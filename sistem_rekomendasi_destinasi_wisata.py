# -*- coding: utf-8 -*-
"""Sistem_Rekomendasi_Destinasi_Wisata.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uukK9_AwhuC2SiqQ5kP4K97pA60hAL7B

# Import Library
"""

!pip install Sastrawi ##Used for stemming for indonesian language

"""Menginstall library sastrawi yang akan digunakan untuk proses stemming"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

from sklearn.feature_extraction.text import TfidfVectorizer
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from sklearn.metrics.pairwise import cosine_similarity

from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path

stem = StemmerFactory().create_stemmer()
stopword = StopWordRemoverFactory().create_stop_word_remover()

"""Menginstall library yang umum digunakan dan inisialisasi stemmer dan stopword removal"""

import os, shutil

"""# Load Data"""

# Import module yang disediakan google colab untuk kebutuhan upload file

from google.colab import files
files.upload()

# Buat direktori jika belum ada

os.makedirs("/root/.kaggle", exist_ok=True)

# Pindahkan file ke direktori .kaggle

shutil.move("kaggle.json", "/root/.kaggle/kaggle.json")

# Atur permission agar tidak terlalu terbuka

os.chmod("/root/.kaggle/kaggle.json", 600)

!kaggle datasets download aprabowo/indonesia-tourism-destination

"""Mendownload dataset dari kaggle dengan perintah diatas"""

!unzip indonesia-tourism-destination.zip

"""unzip folder untuk mengekstrak file di dalamnya"""

tourism_rating = pd.read_csv("tourism_rating.csv")
tourism_with_id = pd.read_csv("tourism_with_id.csv")
user = pd.read_csv("user.csv")

dflist = [tourism_rating, tourism_with_id, user]
for i in dflist:
  display(i)

"""load data yang telah di download dan di ekstrak

# Data Preprocessing
"""

tourism_rating.info()

tourism_with_id.info()

tourism_with_id = tourism_with_id.drop(columns=['Time_Minutes', 'Unnamed: 11', 'Unnamed: 12'])
tourism_with_id.info()

user.info()

"""tiga cell diatas untuk melihat info dari data yang digunakan seperti data type, jumlah entri, dll

## Merge Data Rating dan Tourism with ID
"""

data_recommendation = pd.merge(tourism_rating.groupby('Place_Id')['Place_Ratings'].mean(), tourism_with_id, on='Place_Id')
data_recommendation

"""Melakukan merge data rating dan tourism with id untuk melihat keterkaitan antar variabel nantinya. data ini juga sangat memungkinkan digunakan unutk content based filtering nantinya"""

data_recommendation.info()

"""melihat info data setelah melakukan merge data

# Exploratory Data Analysis

## Melihat Count Traveler berdasarkan Usia
"""

plt.figure(figsize=(10, 5))
sns.countplot(x="Age", data=user, palette='viridis')
plt.title("Count Traveler berdasarkan Usia")
plt.xlabel("Usia")
plt.ylabel("Count")
plt.show

"""dapat dilihat distribusi data berdasarkan jumlah traveler dari usianya. dapat dilihat traveler terbanyak adalah usia 30 tahun

## Melihat Distribusi Lokasi User
"""

plt.figure(figsize=(10, 5))
sns.countplot(x="Location", data=user, palette='viridis')
plt.title("Distribusi Lokasi User")
plt.xlabel("Lokasi")
plt.ylabel("Count")
plt.xticks(rotation=90)
plt.show

"""cell ini unutk melihat penyebaran data lokasi user. sebagian besat user berlokasi di bekasi, semarang, cirebon dan yogyakarta

## Correlation Matrix data_recommendation
"""

# Inisilisasi corr matrix
correlation_matrix = data_recommendation.select_dtypes(include=['float64', 'int64']).corr()

# Plot correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""terlihat beberapa variabel yang memiliki positve correlations dan negative correlations yang cukup kuat"""

# Find the strongest positive and negative correlations
strongest_positive_corr = correlation_matrix.unstack().sort_values().drop_duplicates().tail(5)
strongest_negative_corr = correlation_matrix.unstack().sort_values().drop_duplicates().head(5)

print("Strongest Positive Correlations:")
print(strongest_positive_corr)

print("\nStrongest Negative Correlations:")
print(strongest_negative_corr)

"""berikut adalah uraian correlations terkuat baik itu positive dan negative correlations.

# Data Preparation

## Stemming kalimat, Filter Stopwords & Membuat Variabel 'Tags'
"""

# fungsi untuk preprocessing data
def preprocessing(data):
  data = data.lower()
  data = stem.stem(data)
  data = stopword.remove(data)
  return data

"""fungsi untuk melakukan preprocessing kalimat"""

data_cbf = data_recommendation.copy()
data_cbf['Tags'] = data_cbf['Description'] + ' ' + data_cbf['Category']
data_cbf = data_cbf.drop(columns=['Description', 'Category', 'Price', 'Place_Ratings', 'City'])
data_cbf['Tags'] = data_cbf['Tags'].apply(preprocessing)
data_cbf

"""membuat kolom bari bernama 'Tags' sebagai overview destinasi wisata. variabel ini berisikian value kolom 'Description' dan 'Category'. kemudian menampung data baru di variabel data_cbf yang telah melewati proses drop variabel yang saya anggap perlu untuk di drop karena sudah tidak dibutuhkan.

## TF-IDF Vectorizer
"""

# Inisialisasi TfidfVectorizer
vectorizer = TfidfVectorizer(max_features=5000)

# Mealkukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = vectorizer.fit_transform(data_cbf['Tags']).toarray()

# Menampilkan hasil
print(tfidf_matrix)

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""dua cell diatas digunakan untuk melakukan vectorizer yang sangat berguna karena data kita menggunakan data teks dan kita akan mencari similarity untuk pendekatan content-based filtering

# Modeling Pendekatan Content-Based Filtering

## Cosine Similarity
"""

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""menghitung cosine similarity dari data untuk mendapatkan kesamaan antara data yang nantinya digunakan untuk mendapatkan rekomendasi"""

# membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama tempat wisata
cosine_sim_df = pd.DataFrame(cosine_sim, index=data_cbf['Place_Name'], columns=data_cbf['Place_Name'])
print('Shape:', cosine_sim_df.shape)

# menampilkan cosine_sim_df pada setiap place name
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""membuat datafram dari variabel cosine_sim dengan baris dan kolom berupa tempat wisata. kemudian menampilkan sampel cosine_sim_df"""

def place_recommendations(place_name, similarity_data=cosine_sim_df, items=data_cbf[['Place_Name', 'Tags']], k=5):
    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,place_name].to_numpy().argpartition(range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop place_name agar nama tempat yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(place_name, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""modeling fungsi untuk mendapatkan rekomendasi. dengan menggunakan argpartition, kita mengambil sejumlah nilai k tertinggi dari similarity data. kemudian kita mengambil data dari bobot tertinggi ke terendah. berikutnya kita perlu menghapus place_name yang dicarai agar tidak muncul dalam rekomendasi

## Mendapatkan Rekomendasi
"""

place_recommendations('Pantai Nglambor')

"""terlihat beberapa destinasi wisata yang kita dapatkan sebagai rekomendasi berdasarkan tempat 'Pantai Nglambor'

# Modeling Collaborative Filtering
"""

df = tourism_rating.copy()
df

"""copy touris_rating dan dimasukkan ke variabel df agar data yang digunakan bisa di manipulasi lebih lanjut tanpa mengganggu data sebelumnya

## Data Preparation

### Encoding
"""

# Mengubah User_Id menjadi tanpa nilai yang sama
user_ids = df['User_Id'] .unique().tolist()
print('list User_Id: ', user_ids)

# Melakukan encoding User_Id
user2user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded User_Id : ', user2user_encoded)

# Melakukan proses encoding angka ke User_Id
user_encoded2user = {i: x for i, x in enumerate(user_ids)}
print('encoded angkak ke User_Id: ', user2user_encoded)

"""kita melakuakn encoding pada kolom User_Id, untuk mengubah nilai user ID yang bersifat unik menjadi representasi numerik agar dapat digunakan dalam model machine learning"""

# Mengubah Place_Id menjadi tanpa nilai yang sama
place_ids = df['Place_Id'] .unique().tolist()
print('list Place_Id: ', place_ids)

# Melakukan encoding Place_Id
place2place_encoded = {x: i for i, x in enumerate(place_ids)}
print('encoded Place_Id : ', place2place_encoded)

# Melakukan proses encoding angka ke Place_Id
place_encoded2place = {i: x for i, x in enumerate(place_ids)}
print('encoded angkak ke Place_Id: ', place2place_encoded)

"""hal yang sama dilakukan pada kolom Place_Id untuk mengubah nilai place ID yang bersifat unik menjadi representasi numerik agar dapat digunakan dalam model machine learning"""

# Mapping User_Id ke dataframe user
df['user'] = df['User_Id'].map(user2user_encoded)

# Mapping Place_Id ke dataframe tempat wisata
df['place'] = df['Place_Id'].map(place2place_encoded)
df

"""Kode tersebut digunakan untuk menambahkan kolom baru pada DataFrame df yang berisi hasil encoding dari User_Id dan Place_Id. Melalui map(), kolom User_Id diubah menjadi kolom user menggunakan dictionary user2user_encoded, sedangkan kolom Place_Id diubah menjadi kolom place menggunakan place2place_encoded. Ini penting agar model machine learning, terutama yang berbasis embedding seperti neural collaborative filtering, bisa bekerja dengan data numerik terstruktur alih-alih ID string yang tidak bisa langsung diproses oleh model"""

# Mendapatkan jumlah user
num_users = len(user2user_encoded)
print(num_users)

# Mendapatkan jumlah place name
num_place = len(place2place_encoded)
print(num_place)

# Nilai minimum rating
min_rating = min(df['Place_Ratings'])

# Nilai maksmisal Place_Ratings
max_rating = max(df['Place_Ratings'])

print('Number of User: {}, Number of Place: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_place, min_rating, max_rating
))

"""disini kita mendapatkan informasi dasar mengenai data yang akan digunakan model. pertama num_users dan num_place menghitung jumlah unik pengguna dan tempat wisata berdsarkan hasil encoding sebelumnya. kemudian, min_rating, dan max_rating digunakan unutk mengetahui nilai rating yang diberikan pengguna terhadap tempat wisata. informasi ini penting dalam membangun dan menyesuaikan model, terutama untuk proses normalisasi rating atau sebagai referensi dalam evaluasi peforma model

### Spliting Data
"""

# Mengacak dataset
df = df.sample(frac=1, random_state=42)
df

"""sebelum splitting data kita melakukan pengacakan terlebih dahulu. menginsialisasi random_state agar mendapatkan data yang konsisten"""

# Membuat variabel x untuk mencocokkaan data user dan place menjadi satu value
x = df[['user', 'place']].values

# Membuat variabel y unutk membuat rating
y = df['Place_Ratings'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80:20
train_indices = int(0.8 * df.shape[0])
X_train, X_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:],
)

print(x, y)

"""kita membagi data train dan validation dalam ratio 80:20. yaitu 80 persen data training dan 20 persen data validation

## Training Model
"""

class RecommenderNet(tf.keras.Model):
  # Inisialisasi fungsi
  def __init__(self, num_users, num_place, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_place = num_place
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # Layer embedding user bias
    self.place_embedding = layers.Embedding( # layer embeddings place
        num_place,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.place_bias = layers.Embedding(num_place, 1) # layer embedding place bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:,0]) # memanggil layer embedding 2
    place_vector = self.place_embedding(inputs[:, 1]) # memanggil layer embedding 3
    place_bias = self.place_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_place = tf.tensordot(user_vector, place_vector, 2)

    x = dot_user_place + user_bias + place_bias

    return tf.nn.sigmoid(x)

"""Pada tahap ini, model menghitung skor kecocokan antara pengguna dan tourism place dengan teknik embedding. Pertama, kita melakukan proses embedding terhadap data user dan place. selanjutnya, lakukan operasi perkalian dot product antara embedding user dan place. selain itu, kita juga fapat menambahkan bias unutk setiap user dan place. skor kecocokan diterapkan dalam skala [0,1] dengan fungsi aktivasi sigmoid"""

# Inisialisasi Model
model = RecommenderNet(num_users, num_place, 50)

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""model ini menggunakan Binary Crossentropy unutk menghitung loss function, Adam sebagai optimizer, dan RMSE sebagai metrics evaluation"""

# Train model
history = model.fit(
    x = X_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (X_val, y_val)
)

"""dari hasil training kita dapat rmse yang cukup memuaskan yaitu di angka 0.36

## Evaluasi & Visualisasi
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""dari hasil visualisasi, terdapat skor rmse yang berbeda antara tes dan train. loss sangat rendah pada train dan naik di saat data test. ada kemungkinan model overfitting namun saya rasa cukup memusakan akan hasil ini"""

place_df = data_cbf

# Mengambil sample user
user_id = df.User_Id.sample(1).iloc[0]
place_visited_by_user = df[df.User_Id == user_id]

place_not_visited = place_df[~place_df['Place_Id'].isin(place_visited_by_user.Place_Id.values)]['Place_Id']
place_not_visited = list(
    set(place_not_visited)
    .intersection(set(place2place_encoded.keys()))
)

place_not_visited = [[place2place_encoded.get(x)] for x in place_not_visited]
user_encoder = user2user_encoded.get(user_id)
user_place_array = np.hstack(
    ([[user_encoder]] * len(place_not_visited), place_not_visited)
)

place_df

"""disini kita menyiapkan data input bagi model rekomendasi, dengan fokus pada destinasi wisata yang belum dikunjungi oleh seorang pengguna tertentu."""

ratings = model.predict(user_place_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_place_ids = [
    place_encoded2place.get(place_not_visited[x][0]) for x in top_ratings_indices
]

print("Showing recommendations for users: {}".format(user_id))
print('===' * 9)
print('Place with high ratings from user')
print('----' * 8)

top_place_user = (
    place_visited_by_user.sort_values(
        by = 'Place_Ratings',
        ascending=False
    )
    .head(5)
    .Place_Id.values
)

place_df_rows = place_df[place_df['Place_Id'].isin(top_place_user)]
for row in place_df_rows.itertuples():
  print(row.Place_Name, ':', row.Tags)

print('----' * 8)
print('Top 10 place recommendation')
print('----' * 8)

recommended_place = place_df[place_df['Place_Id'].isin(recommended_place_ids)]
for row in recommended_place.itertuples():
    print(row.Place_Name, ':', row.Tags)

"""dapat dilihat model dapat memberikan rekomendasi yang dipersonalisasi bagi pengguna berdasarkan tempat yang belum pernah di kunjungi"""