# -*- coding: utf-8 -*-
"""Sistem_Rekomendasi_Destinasi_Wisata.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uukK9_AwhuC2SiqQ5kP4K97pA60hAL7B

# Import Library
"""

!pip install Sastrawi ##Used for stemming for indonesian language

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

from sklearn.feature_extraction.text import TfidfVectorizer
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from sklearn.metrics.pairwise import cosine_similarity

from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path

stem = StemmerFactory().create_stemmer()
stopword = StopWordRemoverFactory().create_stop_word_remover()

import os, shutil

"""# Load Data"""

# Import module yang disediakan google colab untuk kebutuhan upload file

from google.colab import files
files.upload()

# Buat direktori jika belum ada

os.makedirs("/root/.kaggle", exist_ok=True)

# Pindahkan file ke direktori .kaggle

shutil.move("kaggle.json", "/root/.kaggle/kaggle.json")

# Atur permission agar tidak terlalu terbuka

os.chmod("/root/.kaggle/kaggle.json", 600)

!kaggle datasets download aprabowo/indonesia-tourism-destination

!unzip indonesia-tourism-destination.zip

tourism_rating = pd.read_csv("tourism_rating.csv")
tourism_with_id = pd.read_csv("tourism_with_id.csv")
user = pd.read_csv("user.csv")

dflist = [tourism_rating, tourism_with_id, user]
for i in dflist:
  display(i)

"""# Data Preprocessing"""

tourism_rating.info()

tourism_with_id.info()

tourism_with_id = tourism_with_id.drop(columns=['Time_Minutes', 'Unnamed: 11', 'Unnamed: 12'])
tourism_with_id.info()

user.info()

"""## Merge Data Rating dan Tourism with ID"""

data_recommendation = pd.merge(tourism_rating.groupby('Place_Id')['Place_Ratings'].mean(), tourism_with_id, on='Place_Id')
data_recommendation

data_recommendation.info()

"""# Exploratory Data Analysis

## Melihat Count Traveler berdasarkan Usia
"""

plt.figure(figsize=(10, 5))
sns.countplot(x="Age", data=user, palette='viridis')
plt.title("Count Traveler berdasarkan Usia")
plt.xlabel("Usia")
plt.ylabel("Count")
plt.show

"""## Melihat Distribusi Lokasi User"""

plt.figure(figsize=(10, 5))
sns.countplot(x="Location", data=user, palette='viridis')
plt.title("Distribusi Lokasi User")
plt.xlabel("Lokasi")
plt.ylabel("Count")
plt.xticks(rotation=90)
plt.show

"""## Correlation Matrix data_recommendation"""

# Inisilisasi corr matrix
correlation_matrix = data_recommendation.select_dtypes(include=['float64', 'int64']).corr()

# Plot correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# Find the strongest positive and negative correlations
strongest_positive_corr = correlation_matrix.unstack().sort_values().drop_duplicates().tail(5)
strongest_negative_corr = correlation_matrix.unstack().sort_values().drop_duplicates().head(5)

print("Strongest Positive Correlations:")
print(strongest_positive_corr)

print("\nStrongest Negative Correlations:")
print(strongest_negative_corr)

"""# Data Preparation

## Stemming kalimat & Membuat Variabel 'Tags'
"""

# fungsi untuk preprocessing data
def preprocessing(data):
  data = data.lower()
  data = stem.stem(data)
  data = stopword.remove(data)
  return data

data_cbf = data_recommendation.copy()
data_cbf['Tags'] = data_cbf['Description'] + ' ' + data_cbf['Category']
data_cbf = data_cbf.drop(columns=['Description', 'Category', 'Price', 'Place_Ratings', 'City'])
data_cbf['Tags'] = data_cbf['Tags'].apply(preprocessing)
data_cbf

"""## TF-IDF Vectorizer"""

# Inisialisasi TfidfVectorizer
vectorizer = TfidfVectorizer(max_features=5000)

# Mealkukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = vectorizer.fit_transform(data_cbf['Tags']).toarray()

# Menampilkan hasil
print(tfidf_matrix)

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""# Modeling Pendekatan Content-Based Filtering

## Cosine Similarity
"""

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

# membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama tempat wisata
cosine_sim_df = pd.DataFrame(cosine_sim, index=data_cbf['Place_Name'], columns=data_cbf['Place_Name'])
print('Shape:', cosine_sim_df.shape)

# menampilkan cosine_sim_df pada setiap place name
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

def place_recommendations(place_name, similarity_data=cosine_sim_df, items=data_cbf[['Place_Name', 'Tags']], k=5):
    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,place_name].to_numpy().argpartition(range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop place_name agar nama tempat yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(place_name, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""## Mendapatkan Rekomendasi"""

place_recommendations('Pantai Nglambor')

"""# Modeling Collaborative Filtering"""

df = tourism_rating.copy()
df

"""## Data Preparation

### Encoding
"""

# Mengubah User_Id menjadi tanpa nilai yang sama
user_ids = df['User_Id'] .unique().tolist()
print('list User_Id: ', user_ids)

# Melakukan encoding User_Id
user2user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded User_Id : ', user2user_encoded)

# Melakukan proses encoding angka ke User_Id
user_encoded2user = {i: x for i, x in enumerate(user_ids)}
print('encoded angkak ke User_Id: ', user2user_encoded)

# Mengubah Place_Id menjadi tanpa nilai yang sama
place_ids = df['Place_Id'] .unique().tolist()
print('list Place_Id: ', place_ids)

# Melakukan encoding Place_Id
place2place_encoded = {x: i for i, x in enumerate(place_ids)}
print('encoded Place_Id : ', place2place_encoded)

# Melakukan proses encoding angka ke Place_Id
place_encoded2place = {i: x for i, x in enumerate(place_ids)}
print('encoded angkak ke Place_Id: ', place2place_encoded)

# Mapping User_Id ke dataframe user
df['user'] = df['User_Id'].map(user2user_encoded)

# Mapping Place_Id ke dataframe tempat wisata
df['place'] = df['Place_Id'].map(place2place_encoded)
df

# Mendapatkan jumlah user
num_users = len(user2user_encoded)
print(num_users)

# Mendapatkan jumlah place name
num_place = len(place2place_encoded)
print(num_place)

# Nilai minimum rating
min_rating = min(df['Place_Ratings'])

# Nilai maksmisal Place_Ratings
max_rating = max(df['Place_Ratings'])

print('Number of User: {}, Number of Place: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_place, min_rating, max_rating
))

"""### Spliting Data"""

# Mengacak dataset
df = df.sample(frac=1, random_state=42)
df

# Membuat variabel x untuk mencocokkaan data user dan place menjadi satu value
x = df[['user', 'place']].values

# Membuat variabel y unutk membuat rating
y = df['Place_Ratings'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80:20
train_indices = int(0.8 * df.shape[0])
X_train, X_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:],
)

print(x, y)

"""## Training Model"""

class RecommenderNet(tf.keras.Model):
  # Inisialisasi fungsi
  def __init__(self, num_users, num_place, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_place = num_place
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # Layer embedding user bias
    self.place_embedding = layers.Embedding( # layer embeddings place
        num_place,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.place_bias = layers.Embedding(num_place, 1) # layer embedding place bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:,0]) # memanggil layer embedding 2
    place_vector = self.place_embedding(inputs[:, 1]) # memanggil layer embedding 3
    place_bias = self.place_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_place = tf.tensordot(user_vector, place_vector, 2)

    x = dot_user_place + user_bias + place_bias

    return tf.nn.sigmoid(x)

# Inisialisasi Model
model = RecommenderNet(num_users, num_place, 50)

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Train model
history = model.fit(
    x = X_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (X_val, y_val)
)

"""## Evaluasi & Visualisasi"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

place_df = data_cbf

# Mengambil sample user
user_id = df.User_Id.sample(1).iloc[0]
place_visited_by_user = df[df.User_Id == user_id]

place_not_visited = place_df[~place_df['Place_Id'].isin(place_visited_by_user.Place_Id.values)]['Place_Id']
place_not_visited = list(
    set(place_not_visited)
    .intersection(set(place2place_encoded.keys()))
)

place_not_visited = [[place2place_encoded.get(x)] for x in place_not_visited]
user_encoder = user2user_encoded.get(user_id)
user_place_array = np.hstack(
    ([[user_encoder]] * len(place_not_visited), place_not_visited)
)

place_df

ratings = model.predict(user_place_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_place_ids = [
    place_encoded2place.get(place_not_visited[x][0]) for x in top_ratings_indices
]

print("Showing recommendations for users: {}".format(user_id))
print('===' * 9)
print('Place with high ratings from user')
print('----' * 8)

top_place_user = (
    place_visited_by_user.sort_values(
        by = 'Place_Ratings',
        ascending=False
    )
    .head(5)
    .Place_Id.values
)

place_df_rows = place_df[place_df['Place_Id'].isin(top_place_user)]
for row in place_df_rows.itertuples():
  print(row.Place_Name, ':', row.Tags)

print('----' * 8)
print('Top 10 place recommendation')
print('----' * 8)

recommended_place = place_df[place_df['Place_Id'].isin(recommended_place_ids)]
for row in recommended_place.itertuples():
    print(row.Place_Name, ':', row.Tags)